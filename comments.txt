----------------------- REVIEW 1 ---------------------
Moving beyond building a classifier with better features or more/different 
data, I would recommend the primary author to think deeply about the relevance 
of the problem and how it informs (a) the interpretation of your current results 
and (b) direction of your next steps.

---------
Response
---------
	We appreciate your suggestion. This is what we think of the result and 
	plan to do next.
	
	Excerpt from the Conclusion section of our paper:
	`'The current $F_1$ scores are still markedly low. Better modeling of the 
	instructor may help boost performance. We plan to tackle this in two ways. 
	First, we will model instructor intervention based on the threads they have 
	seen because they cannot intervene in threads that they have not seen. Second, 
	we will model intervention based on the role that different types of 
	instructional staff play.''
	
	
Some thoughts, questions and comments that might help follow:

1. Why is the work specifically situated in the context of MOOCs? Isn't this 
problem and its solution more broadly applicable to educational discussion forum 
or even moderated discussion forums in general?

---------
Response
---------

	The problem of moderating web forums have been studied before [1,2]. However, 
	PDTB discourse connectives occur at sentence / clause level. This requires 
	well-structured grammatical sentences to occur in the corpus. MOOC forum posts 
	satisfy this criteria much better than social media and web forum posts. 
	Although MOOC forum posts are user generated, the text in more similar to 
	those in a student essay corpus than those on social media or web forums.
	This motivates our methodology on this dataset.
	
	However, we agree that the problem and our proposed method are suitable for 
	other online learning environments besides MOOCs such as forums hosted by 
	learning management systems (LMS) by universities for their own students and 
	the industry for internal training.

	 1) L. Wang, S. N. Kim, and T. Baldwin. The Utility of
	Discourse Structure in Identifying Resolved Threads in
	Technical User Forums. In Proc. of COLING ?12 , pages 2739?2756, 2012
	- This work refers to the discourse in the thread structure sense, not 
	sentence / clause level discourse we investigate.

	 2) L. Backstrom, J. Kleinberg, L. Lee, and C. Danescu-Niculescu-Mizil. Characterizing 
	and curating conversation threads: expansion, focus, volume, re-entry. In Proc. of 
	WSDM ?13, pages 13?22. ACM, 2013
	- This work is on social media threads.

2. In its current formulation, the prediction task seems to be if a human 
instructor will intervene in a discussion thread. Caveat of predicting only 
the first intervention is reasonable. However, considering the application 
scenarios, would it not be necessary to not only predict if a human instructor 
will intervene, but also (approximately) when? I think it would be useful to include 
con-ops for use of this prediction module to augment discussion forums where 
moderators are heavily burdened.

---------
Response
---------
	We agree that an ideal solution to the scenario warrants a system that can predict
	interventions at a post level, that is predict every intervention, rather than at a 
	thread level. However, our analysis indicates the problem if predicting 2nd and 
	successive interventions or reintervention, is different than the problem we are 
	handling currently. Not to say that discourse connectives won't be useful then. 
	This is because the first intervention creates a bias by making the intervened thread
	more visible among the students and instructors. Further, reintervention prediction 
	also needs to consider if the first intervention moved the thread towards a closure 
	or otherwise. This move also depends on the nature of the subject area (humanities vs 
	hard sciences).	So, we do not handle it in this work.

3. Table 2: Is the distribution of tags for MOOC corpus based on tags 
generated by discourse parser or through manual labeling? If the former, 
the possibility of the discourse parser being biased to match its training 
distribution of tags should be considered? I don't think this table makes a 
valuable point that is not made elsewhere in that section.

---------
Response
---------
	Yes, the distribution of tags for MOOC corpus are based on tags generated 
	by the discourse parser. While we acknowledge the possibility being pointed 
	out, we cite several earlier that have used the same parser without 
	retraining.	For example, we cite Faulkner et al. (2014), Wang et al (2012), 
	Swanson et al (2015) in our related work. They all use the parser on non-news 
	corpora without retraining and report positive results. Not retraining is 
	reasonable since retraining  requires needs substantial annotated corpus to be 
	created with much effort without any knowledge of the parser's utility for the task. 
	Now that we know the features are useful, we will consider creating an annotated MOOC
	corpus in our future work.
	
4. Table 3: Why are some of the rows zeros? Please double check if this is an error.

---------
Response
---------
	We appreciate your observation. However, this is not in error. 
	In Table 2 (in-domain evaluation) CLASSIC-2 and DISASTER-3 rows have 0 F_1 
	score on both models. From Table one can observe that both these courses 
	have very few interventions (17 for CLASSIC-2 and 18 for DISASTER-3). 
	During a 5 fold cross validation evaluation the number of interventions in 
	each fold is further diminished (~ 3 to 4 per fold). The two models are 
	unable to predict any intervention for both the course in the in-domain 
	evaluation.

5. It appears precision is low-to-medium in most cases. One possibility may be 
that the classifier is over predicting intervention labels. Would be useful to 
include the following information: (a) How many (#) thread does the classifier 
predict a human intervention? (b) How is the model's operating points selected?

---------
Response
---------
	We agree. All our models have a high false positive rate (FPR). We could add  
	this statistic to the final version of the paper. However, for the problem of 
	intervention prediction, recall is more important than precision since it is 
	costlier (in terms of loss in student learning) to miss threads that were 
	intervened by the human instructor than to predict additional threads that 
	weren't originally intervened by the human instructor.

6. Table 5: Macro Avg. row has an error (0.6 should be 2.6)

---------
Response
---------
	We agree and regret the typo. Thanks for spotting the significant error!

7. One of the contributions in your paper is that of the technique to use 
out-domain cross validation. In all experimental setups, on average this 
leads to improvements. Most interesting is the result noticeable across 
table 3 & 4 for E+P where macro and weight macro averages increase by 5.4% 
and 5.7% respectively. This is the most substantial improvement in your results 
(in vs. out comparison for PDTB does not count because its the lowest baseline). 
However, this improvement is not even explicitly noted in your write up.

---------
Response
---------
    Yes. we agree that this is a key result of the paper. We compare the gains by E+P 
	and EDM'15 in the out-of-domain setting over its performance in the in-domain-setting 
	on page 5, para 2 below Table 4. We specify improvement in weighted macro average.
    
	We state that, ''The EDM'15 model only improved by 4.3%, whereas the E+P model improved 
	by 6.9%'' This sentence should instead read: ''The EDM'15 model only improved by 2.7%, 
	whereas the E+P model improved by 5.7%''. We regret this error.

----------------------- REVIEW 2 ---------------------
The discussion of research question 2 and Table 5 is somewhat confusing. Is 
the comparison between EDM'15 and EDM'15+PDTB like the rest of the paper?

---------
Response
---------	
	No. The comparison in Table 5 and research question 2 is between EDM 
	and PDTB model. The PDTB model is one with just the PDTB features (25 in 
	number) without the EDM'15 model. Our main results in Tables 3 and 4 
	compare EDM'15 and EDM'15+PDTB models.
	Research question 2 discusses the power of PDTB features over vocabulary 
	based features (EDM'15 based models. The comparison is made in this light.

This is more of a suggestion on the format. The authors could consider add a 
paragraph towards the end of the Introduction session to discuss the novelty 
of the work and contributions.

---------
Response
---------
	Thanks for your suggestion! We think it makes perfect sense to do so. Due 
	to space constraints we limited ourselves to the conclusion and abstract to 
	highlight our contribution. They are:
		
		`'In this study, we better the state-of-the-art for intervention prediction 
		by augmenting it with PDTB relation based features. Further, on select MOOC 
		offerings PDTB relations alone performed comparably to the state-of-the-art.
		Unlike vocabulary based models, PDTB based features were shown to be robust 
		to domain differences across MOOCs. This domain independence supports the 
		improved prediction of instructor interventions.''
		
	We will add these in the camera ready version of the paper due to AAAI's 
	relatively liberal policy on space in the camera ready version.

----------------------- REVIEW 3 ---------------------

I am curious to learn more about the precise discourse relations that are most relevant.
Also, the application seems a bit limited. Could these techniques and insight also 
be relevant for other applications in the field?

---------
Response
---------
	Thanks for asking! We had analysed the contributions of various features 
	in our models. 	We find 'Contingency' and 'Expansion' relations to contribute 
	the most. We believe that this is due to the higher prevalence relative to 
	the other discourse relations in the corpus. We will make space in the camera 
	ready version of the paper to add this.